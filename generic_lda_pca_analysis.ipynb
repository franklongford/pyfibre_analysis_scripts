{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Cancer Biopsy Image Data\n",
    "\n",
    "This notebook presents the analysis of pandas databases, each containing PyFibre output data for multiple biopsy images. The databases are labelled according to the directories that they are located in, for example \"Normal\", \"In Situ\", \"Misto\" and \"Solido - Invasor\". \n",
    "\n",
    "The notebook performs feature selection based on Linear Discriminant Analysis (LDA) as well as Principle Component Analysis (PCA) on the image metrics of each image.\n",
    "\n",
    "To begin with we load in all the libraries that we will need, including pandas database (http://pandas.pydata.org/), and scikit-learn machine learning (https://scikit-learn.org/stable/) packages. Both are standard data handling packages for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFECV, f_classif\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import (\n",
    "    auc, roc_curve, confusion_matrix, accuracy_score, classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a useful plotting function for a viewing the first two dimensions of a multi-dimensional array as a scatter plot coloured by labels. The size of the points can be determined by the probability of each assigned label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_tools import scatter, load_databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in the databases for Normal, In Situ, Misto and Solido - Invasor labelled images. We can change the extension on each database file to load in just the fibre or cell segment databases. At the moment, we only load in the global image metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "# Extension options to load each type of PyFibre database\n",
    "extensions = ['_global', '_fibre', '_cell', '_network']\n",
    "\n",
    "# Enter in the names of directories here\n",
    "directories = []\n",
    "data_directories = [\n",
    "    os.path.join(current_dir, group) + '/' for group in directories\n",
    "]\n",
    "\n",
    "filename = 'pyfibre_database'\n",
    "database = load_databases(filename + extensions[0], data_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tidy up the databases to remove any entries with missing values . For cell and fibre databases, we only retain the 4 largest regions for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_cut = database.copy()\n",
    "groups = np.unique(database['Label'])\n",
    "\n",
    "try: \n",
    "    database_cut = database_cut.where(database_cut['ID'] <= 0)\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try: \n",
    "    database_cut = database_cut.where(database_cut['Cell Segment Area'] >= 200)\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try: \n",
    "    database_cut = database_cut.where(database_cut['Fibre Segment Area'] >= 200)\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try: \n",
    "    database_cut = database_cut.where(database_cut['No. Fibres'] >= 10)\n",
    "except: \n",
    "    pass\n",
    "        \n",
    "# For now we remove the 'ID' entries for simplicity\n",
    "try: \n",
    "    database_cut = database_cut.drop(['File', 'ID'], axis=1)\n",
    "except: \n",
    "    database_cut = database_cut.drop(['File'], axis=1)\n",
    "\n",
    "database_cut = database_cut.replace(0, np.nan)\n",
    "database_cut = database_cut.replace(np.inf, np.nan)\n",
    "database_cut = database_cut.dropna()\n",
    "\n",
    "# Map each seperate group name to the new database\n",
    "database_cut['Group'] = database_cut['Label'].map(\n",
    "    {float(index + 1): value for index, value in enumerate(groups)})\n",
    "database_cut['Group'] = pd.Categorical(\n",
    "    database_cut['Group'], categories=groups, ordered=True)\n",
    "\n",
    "# Display an excert from the total database\n",
    "database_cut.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a normalised database where each metric ranges between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(database_cut.columns)\n",
    "columns.remove('Label')\n",
    "columns.remove('Group')\n",
    "\n",
    "df_norm = database_cut.copy()\n",
    "x = df_norm[columns].values\n",
    "\n",
    "# StandardScalar scales all values between -1 to 1\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "# QuantileTransformer enforces a Gaussian distribution of variables\n",
    "x_scaled = QuantileTransformer(output_distribution='normal').fit_transform(x_scaled)\n",
    "\n",
    "df_norm[columns] = x_scaled\n",
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view boxplots of each metric corresponding to each labelled group. These are much easier to display side-by-side if they are normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.boxplot(column=columns, by=['Group'], figsize=(25, 25))\n",
    "plt.savefig('full_metric_boxplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to use LDA to use as our estimator. Feature ranking with recursive feature elimination and cross-validation (RFECV) can be used to reduce the number of metrics in our dataset. We are then left with a metric selection containing the best number of features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(database, labels, n_splits=6, n_repeats=8):\n",
    "            \n",
    "    print(\"Total number of raw metrics = {}\".format(database.shape[1]))\n",
    "    X = database.values\n",
    "    y = labels.values\n",
    "    \n",
    "    kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=None)    \n",
    "    lda = LinearDiscriminantAnalysis(solver='eigen', store_covariance=True)\n",
    "    rfecv = RFECV(estimator=lda, step=1, cv=kfold, scoring='accuracy')\n",
    "    \n",
    "    rfecv.fit(X, y)\n",
    "\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)    \n",
    "    print(\"Features selected: \", database.loc[:, rfecv.support_].columns)\n",
    "    print(\"Features dropped: \", database.loc[:, ~rfecv.support_].columns)\n",
    "    \n",
    "    # Drops features\n",
    "    features = database.loc[:, rfecv.support_]\n",
    "\n",
    "    text_size = 14\n",
    "    mpl.rc('xtick', labelsize=text_size) \n",
    "    mpl.rc('ytick', labelsize=text_size) \n",
    "    mpl.rcParams.update({'font.size': text_size})\n",
    "    \n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure(0, figsize=(8,6))\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (accuracy %)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), 100 * rfecv.grid_scores_)\n",
    "    plt.savefig(f'cv_scores_lda.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_norm['Label'].copy()\n",
    "\n",
    "df_full = df_norm.drop(['Group', 'Label'], axis=1)\n",
    "\n",
    "df_red = feature_selection(df_full, labels)                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Linear Discriminant Analysis (LDA) on image metric database using labels supplied by medics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_analysis(database, labels, tick_labels=None, n_splits=6, n_repeats=10, tag=''):\n",
    "    \n",
    "    X = database.values\n",
    "    y = labels.values\n",
    "    \n",
    "    kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=None)\n",
    "    lda = LinearDiscriminantAnalysis(solver='eigen', store_covariance=True)\n",
    "    \n",
    "    scores = []\n",
    "    binary_scores = []\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test = X[test_index], y[test_index]\n",
    "        \n",
    "        lda.fit(X_train, y_train)\n",
    "        y_predict = lda.predict(X_test)\n",
    "\n",
    "        roc_y = np.where(y_test == 1, 1, 0)\n",
    "        roc_y_test = np.where(y_predict == 1, 1, 0)\n",
    "        \n",
    "        binary_scores.append(accuracy_score(roc_y, roc_y_test))\n",
    "        scores.append(accuracy_score(y_test, y_predict))\n",
    "    \n",
    "    print(\"Average cross validation score for all classes: \", np.mean(scores), \"+= \", np.std(scores))\n",
    "    print(\"Average cross validation score for Normal vs other classes: \", np.mean(binary_scores), \"+= \", np.std(binary_scores))\n",
    "    \n",
    "    # Plotting the \n",
    "    X_plot_train = lda.transform(X_train)\n",
    "    X_plot_test = lda.transform(X_test)\n",
    "    \n",
    "    plt.figure(0)\n",
    "    fig, ax, cb = scatter(X_plot_train, y_train, np.ones(y_train.shape))\n",
    "    fig, ax, cb = scatter(X_plot_test, y_test, np.ones(y_test.shape),\n",
    "                          ellipse=False, marker='x', alpha=0.6, fig=fig, ax=ax, cb=cb)\n",
    "    cb.set_ticklabels(tick_labels)\n",
    "    #plt.axis('off')\n",
    "    #plt.axis([-5, 5, -5, 5])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'lda_{tag}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    im = ax.matshow(lda.coef_, interpolation='none', cmap='coolwarm')\n",
    "    fig.colorbar(im)\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(database.columns)))\n",
    "    ax.set_xticklabels(list(database.columns))\n",
    "    ax.set_yticks(np.arange(len(tick_labels)))\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "    \n",
    "    plt.setp([tick.label2 for tick in ax.xaxis.get_major_ticks()], rotation=45,\n",
    "         ha=\"left\", va=\"center\",rotation_mode=\"anchor\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    im = ax.matshow(lda.covariance_, interpolation='none', cmap='coolwarm')\n",
    "    fig.colorbar(im)\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(database.columns)))\n",
    "    ax.set_xticklabels(list(database.columns))\n",
    "    ax.set_yticks(np.arange(len(database.columns)))\n",
    "    ax.set_yticklabels(list(database.columns))\n",
    "    \n",
    "    plt.setp([tick.label2 for tick in ax.xaxis.get_major_ticks()], rotation=45,\n",
    "         ha=\"left\", va=\"center\",rotation_mode=\"anchor\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_labels = labels # Change this if you want to use different labels for the figures (i.e. use English spellings)\n",
    "\n",
    "print('Using full set of PyFibre Metrics')\n",
    "lda_analysis(df_full, labels, tick_labels=tick_labels, tag='full')\n",
    "\n",
    "print('Using reduced set of PyFibre Metrics')\n",
    "lda_analysis(df_red, labels, tick_labels=tick_labels, tag='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform PCA analysis on the databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_analysis(database, labels, tick_labels, per_var=0.9, supervised=False):\n",
    "\n",
    "    (X_train, X_test, \n",
    "         y_train, y_test) = train_test_split(database, labels, train_size=0.8)\n",
    "    \n",
    "    pca = PCA(n_components=per_var, whiten=True, svd_solver='full')\n",
    "    \n",
    "    print(\"Total number of raw metrics = {}\".format(database.shape[1]))\n",
    "    print(database.columns)\n",
    "    \n",
    "    print(\"Training PCA on set of {} images\".format(X_train.shape[0]))\n",
    "    \n",
    "    X = pca.fit_transform(X_train)\n",
    "\n",
    "    plt.plot(pca.explained_variance_ratio_.cumsum())\n",
    "    plt.show()\n",
    "    \n",
    "    n_components = len(pca.explained_variance_)\n",
    "    print(\"PCA components retained = {}\".format(n_components))\n",
    "\n",
    "    y = np.array(labels)\n",
    "\n",
    "    plt.figure(1)\n",
    "    fig, cb = scatter(X, y_train, np.ones(y_train.shape))\n",
    "    cb.set_ticklabels(tick_labels)\n",
    "    plt.axis('off')\n",
    "    plt.axis([-5, 5, -5, 5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Performing PCA transform on test set of {} images\".format(X_test.shape[0]))\n",
    "\n",
    "    plt.figure(1)\n",
    "    fig, cb = scatter(pca.transform(X_test), y_test, np.ones(y_test.shape))\n",
    "    cb.set_ticklabels(groups)\n",
    "    plt.axis('off')\n",
    "    plt.axis([-5, 5, -5, 5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\"\n",
    "    for i, label in enumerate(tick_labels):\n",
    "        plt.figure()\n",
    "        fig, cb = scatter(X, y, np.where(y == i + 1, 1, 0))\n",
    "        cb.set_ticklabels(tick_labels)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using full set of PyFibre Metrics')\n",
    "pca_analysis(df_full, labels, tick_labels=tick_labels)\n",
    "\n",
    "print('Using reduced set of PyFibre Metrics')\n",
    "pca_analysis(df_red, labels, tick_labels=tick_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
