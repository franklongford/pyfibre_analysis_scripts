{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Cancer Biopsy Image Data\n",
    "\n",
    "This notebook presents the analysis of pandas databases, each containing PyFibre output data for multiple biopsy images. The databases are labelled according to the directories that they are located in.\n",
    "\n",
    "The notebook performs feature selection based on Linear Discriminant Analysis (LDA) as well as Principle Component Analysis (PCA) on the image metrics of each image.\n",
    "\n",
    "To begin with we load in all the libraries that we will need, including pandas database (http://pandas.pydata.org/), and scikit-learn machine learning (https://scikit-learn.org/stable/) packages. Both are standard data handling packages for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "# Set plotting options for the notebook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "text_size = 14\n",
    "mpl.rc('xtick', labelsize=text_size) \n",
    "mpl.rc('ytick', labelsize=text_size) \n",
    "mpl.rcParams.update({'font.size': text_size})\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFECV, f_classif\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import (\n",
    "    auc, roc_curve, confusion_matrix, accuracy_score, classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a useful plotting function for a viewing the first two dimensions of a multi-dimensional array as a scatter plot coloured by labels. The size of the points can be determined by the probability of each assigned label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfibre_analysis_tools import scatter, load_databases, plot_lda_analysis, plot_roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in the databases for all labelled images. We can change the extension on each database file to load in just the fibre or cell segment databases. At the moment, we only load in the global image metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "# Extension options to load each type of PyFibre database\n",
    "extensions = ['_global', '_fibre', '_cell', '_network']\n",
    "\n",
    "# Enter in the names of directories here. By default these are also chosen as the \n",
    "# grading labels for each image\n",
    "directories = []\n",
    "data_directories = [\n",
    "    os.path.join(current_dir, group) for group in directories\n",
    "]\n",
    "\n",
    "filename = 'pyfibre_database'\n",
    "database = load_databases(filename + extensions[0], data_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tidy up the databases to remove any entries with missing values . For cell and fibre databases, we only retain the 4 largest regions for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_cut = database.copy()\n",
    "groups = np.unique(database['Label'])\n",
    "\n",
    "# Only select the largest segements if loading in Cell / Fibre segment databases\n",
    "n_segments = 4\n",
    "try: \n",
    "    database_cut = database_cut.where(database_cut['ID'] <= n_segments-1)\n",
    "    database_cut = database_cut.drop(['ID'], axis=1)\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Filter segment metrics based on a minimum pixel area\n",
    "min_area = 200\n",
    "try: \n",
    "    database_cut = database_cut.where(database_cut['Cell Segment Area'] >= min_area)\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try: \n",
    "    database_cut = database_cut.where(database_cut['Fibre Segment Area'] >= min_area)\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Filter metrics based on a minimum number of fibres\n",
    "min_fibres = 10\n",
    "try: \n",
    "    database_cut = database_cut.where(database_cut['No. Fibres'] >= min_fibres)\n",
    "except: \n",
    "    pass\n",
    "        \n",
    "# Remove any 'File' entries for simplicity\n",
    "database_cut = database_cut.drop(['File'], axis=1)\n",
    "\n",
    "# Remove any Nan / zero / infinite values from the database\n",
    "database_cut = database_cut.replace(0, np.nan)\n",
    "database_cut = database_cut.replace(np.inf, np.nan)\n",
    "database_cut = database_cut.dropna()\n",
    "\n",
    "# Map each seperate group name to the new database\n",
    "database_cut['Group'] = database_cut['Label'].map(\n",
    "    {float(index + 1): value for index, value in enumerate(groups)})\n",
    "database_cut['Group'] = pd.Categorical(\n",
    "    database_cut['Group'], categories=groups, ordered=True)\n",
    "\n",
    "# Display an excert from the new database\n",
    "database_cut.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a normalised database where each metric ranges between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(database_cut.columns)\n",
    "columns.remove('Label')\n",
    "columns.remove('Group')\n",
    "\n",
    "# Extract any numerical values\n",
    "df_norm = database_cut.copy()\n",
    "x = df_norm[columns].values\n",
    "\n",
    "# StandardScalar scales all values between -1 to 1\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "# QuantileTransformer enforces a Gaussian distribution of variables\n",
    "x_scaled = QuantileTransformer(output_distribution='normal').fit_transform(x_scaled)\n",
    "\n",
    "# Replace the scaled numerical values back in\n",
    "df_norm[columns] = x_scaled\n",
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view boxplots of each metric corresponding to each labelled group. These are much easier to display side-by-side if they are normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.boxplot(column=columns, by=['Group'], figsize=(25, 25))\n",
    "plt.savefig('full_metric_boxplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to use LDA to use as our estimator. Feature ranking with recursive feature elimination and cross-validation (RFECV) can be used to reduce the number of metrics in our dataset. We are then left with a metric selection containing the best number of features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(database, labels, n_splits=6, n_repeats=8):\n",
    "    \"\"\"Retain significant features based on cross validation score of LDA\"\"\"\n",
    "    \n",
    "    print(\"Total number of raw metrics = {}\".format(database.shape[1]))\n",
    "    X = database.values\n",
    "    y = labels.values\n",
    "    \n",
    "    kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=None)    \n",
    "    lda = LinearDiscriminantAnalysis(solver='eigen', store_covariance=True)\n",
    "    rfecv = RFECV(estimator=lda, step=1, cv=kfold, scoring='accuracy')\n",
    "    \n",
    "    rfecv.fit(X, y)\n",
    "\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)    \n",
    "    print(\"Features selected: \", database.loc[:, rfecv.support_].columns)\n",
    "    print(\"Features dropped: \", database.loc[:, ~rfecv.support_].columns)\n",
    "    \n",
    "    # Drops features with low significance\n",
    "    features = database.loc[:, rfecv.support_]\n",
    "    \n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure(0, figsize=(8,6))\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (accuracy %)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), 100 * rfecv.grid_scores_)\n",
    "    plt.savefig(f'cv_scores_lda.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_norm['Label'].copy()\n",
    "\n",
    "df_full = df_norm.drop(['Group', 'Label'], axis=1)\n",
    "\n",
    "df_red = feature_selection(df_full, labels)                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Linear Discriminant Analysis (LDA) on image metric database using labels supplied by medics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_analysis(database, labels, tick_labels=None, n_splits=6, n_repeats=10, tag=''):\n",
    "    \n",
    "    X = database.values\n",
    "    y = labels.values\n",
    "    \n",
    "    kfold = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits, n_repeats=n_repeats, random_state=None)\n",
    "    lda = LinearDiscriminantAnalysis(\n",
    "        solver='eigen', store_covariance=True)\n",
    "    \n",
    "    scores = []\n",
    "    binary_scores = []\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test = X[test_index], y[test_index]\n",
    "        \n",
    "        lda.fit(X_train, y_train)\n",
    "        y_predict = lda.predict(X_test)\n",
    "\n",
    "        roc_y_test = np.where(y_test == 1, 1, 0)\n",
    "        roc_y_predict = np.where(y_predict == 1, 1, 0)\n",
    "        \n",
    "        binary_scores.append(accuracy_score(roc_y_test, roc_y_predict))\n",
    "        scores.append(accuracy_score(y_test, y_predict))\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(roc_y_test, roc_y_predict)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(roc_auc)\n",
    "    \n",
    "    print(\"Average cross validation score for all classes: \", np.mean(scores), \"+= \", np.std(scores))\n",
    "    print(\"Average cross validation score for Normal vs other classes: \", np.mean(binary_scores), \"+= \", np.std(binary_scores))\n",
    "    \n",
    "    plot_lda_analysis(\n",
    "        lda, (X_train, y_train), (X_test, y_test), \n",
    "        columns=database.columns, \n",
    "        tick_labels=tick_labels,\n",
    "        tag=tag\n",
    "    )\n",
    "    plot_roc_curve(tprs, aucs, tag=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_labels = directories # Change this if you want to use different labels for the figures (i.e. use English spellings)\n",
    "\n",
    "print('Using full set of PyFibre Metrics')\n",
    "lda_analysis(df_full, labels, tick_labels=tick_labels, tag='full')\n",
    "\n",
    "print('Using reduced set of PyFibre Metrics')\n",
    "lda_analysis(df_red, labels, tick_labels=tick_labels, tag='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform PCA analysis on the databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_analysis(pca, training, test, tick_labels, tag=''):\n",
    "    \n",
    "    # Plotting the training and test sets for an example LDA\n",
    "    X_plot_train = pca.transform(training[0])\n",
    "    X_plot_test = pca.transform(test[0])\n",
    "    \n",
    "    plt.figure(0)\n",
    "    fig, ax, cb = scatter(X_plot_train, training[1], np.ones(training[1].shape))\n",
    "    fig, ax, cb = scatter(X_plot_test, test[1], np.ones(test[1].shape),\n",
    "                          ellipse=False, marker='x', alpha=0.6, fig=fig, ax=ax, cb=cb)\n",
    "    cb.set_ticklabels(tick_labels)\n",
    "    plt.axis('off')\n",
    "    plt.axis([-5, 5, -5, 5])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'pca_{tag}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pca_analysis(database, labels, tick_labels, per_var=0.9, supervised=False, tag=''):\n",
    "\n",
    "    (X_train, X_test, \n",
    "         y_train, y_test) = train_test_split(database, labels, train_size=0.8)\n",
    "    \n",
    "    pca = PCA(n_components=per_var, whiten=True, svd_solver='full')\n",
    "    \n",
    "    print(\"Total number of raw metrics = {}\".format(database.shape[1]))\n",
    "    print(database.columns)\n",
    "    \n",
    "    print(\"Training PCA on set of {} images\".format(X_train.shape[0]))\n",
    "    print(\"Performing PCA transform on test set of {} images\".format(X_test.shape[0]))\n",
    "\n",
    "    X = pca.fit_transform(X_train)\n",
    "    plt.plot(pca.explained_variance_ratio_.cumsum())\n",
    "    plt.show()\n",
    "    \n",
    "    n_components = len(pca.explained_variance_)\n",
    "    print(\"PCA components retained = {}\".format(n_components))\n",
    "    \n",
    "    plot_pca_analysis(\n",
    "        pca, (X_train, y_train), (X_test, y_test), \n",
    "        tick_labels=tick_labels, tag=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using full set of PyFibre Metrics')\n",
    "pca_analysis(df_full, labels, tick_labels=tick_labels, tag='full')\n",
    "\n",
    "print('Using reduced set of PyFibre Metrics')\n",
    "pca_analysis(df_red, labels, tick_labels=tick_labels, tag='red')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
